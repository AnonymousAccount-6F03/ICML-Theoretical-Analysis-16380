**[Supplementary materials on the theoretical proof of Submission 16380]**

# Theoretical Analysis: Impact of Distance and Similarity Constraints on Navigation Policy Optimization in PPO

This section provides a theoretical analysis of why incorporating "Distance Constraint" and "Similarity Constraint" into the PPO training process helps the agent learn better navigation policies. The analysis is based on the concept of objective function modification in Markov Decision Processes (MDPs) and the influence on policy gradient updates. We explain how reducing these constraint terms leads to policy improvements from the perspective of loss function modification and gradient influence in reinforcement learning.

## 1. Problem Formulation and Background

Let the reinforcement learning task be described by a Markov Decision Process (MDP), denoted as $\mathcal{M} = (\mathcal{S}, \mathcal{A}, P, r, \gamma)$, where:

- $\mathcal{S}$ is the state space, including current environmental observations (such as image features) and the agent's pose in the environment;
- $\mathcal{A}$ is the action space, representing the actions available to the agent;
- $P(s_{t+1} \mid s_t, a_t)$ is the state transition probability;
- $r(s_t, a_t)$ is the intrinsic immediate reward function;
- $\gamma \in (0,1)$ is the discount factor.

The objective of PPO is to learn a policy $\pi_\theta(a \mid s)$ by maximizing the expected cumulative reward:

$$
J(\theta) = ùîº _{s _0,a _0,\dots} \left[\sum _{t=0}^{\infty} \gamma^t \cdot r(s _t,a _t) \right]
$$

While constraining updates between the old policy $\pi_{\theta_{\text{old}}}$ and the new policy $\pi_{\theta}$ to prevent policy collapse.

### 1.1 Distance and Similarity Constraints

We introduce two additional constraint (loss) terms:

- **Distance Loss** $L_{\mathrm{dist}}$: Represents the deviation between the agent's minimum distance from surrounding obstacles and a safety threshold, or a negative measure of the desired safety distance. When smaller, it indicates the agent maintains a larger margin from obstacles, resulting in safer navigation.
- **Similarity Loss** $L_{\mathrm{sim}}$: Represents the degree of difference between the current observation/pose and the target (or sub-goal) image features. When smaller, it indicates the agent is closer to the target state distribution, leading to more efficient goal approach.

By incorporating these additional loss terms as "negative rewards" or "constraint penalties" into the original PPO loss function (or integrating them into the original reward through multi-task reward weighting), the policy optimization objective becomes:

$$
\max _\theta \widetilde{J}(\theta) = ùîº \left[ \sum _{t=0}^{\infty} \gamma^t \left(r(s _t,a _t)
-\beta _1 \cdot L _{\mathrm{dist}}(s _t,a _t)
-\beta _2 \cdot L _{\mathrm{sim}}(s _t,a _t) \right) \right]
$$

where $\beta _1$ and $\beta _2$ are hyperparameters that balance the intrinsic rewards and constraint penalties.

## 2. Theoretical Analysis

### 2.1 Sufficient Conditions for Policy Improvement

In policy gradient frameworks (including PPO), the direction of policy updates can generally be represented as:

$$
\nabla _\theta \widetilde{J}(\theta) \approx ùîº _{s,a \sim \pi _\theta} \left[\nabla _\theta \log \pi _\theta(a \mid s), \widetilde{A}^\pi(s,a) \right]
$$

where $\widetilde{A}^\pi(s,a)$ is the new advantage function that incorporates modifications from the additional penalty terms.

Let:

$$
\widetilde{Q}^\pi(s,a) = ùîº\left[\sum_{k=0}^\infty \gamma^k \bigl(r(s_{t+k},a_{t+k})
-\beta_1 \cdot L_{\mathrm{dist}}(s_{t+k},a_{t+k})
-\beta_2 \cdot L_{\mathrm{sim}}(s_{t+k},a_{t+k}) \bigr)\right]
$$

Then:

$$
\widetilde{A}^\pi(s,a) = \widetilde{Q}^\pi(s,a) - \widetilde{V}^\pi(s)
$$

where $\widetilde{V}^\pi(s)$ is the corresponding state value function. If certain state-action pairs $(s,a)$ incur high constraint penalties (i.e., large distance or similarity losses) during updates, then $\widetilde{Q}^\pi(s,a)$ decreases, resulting in a smaller advantage function, thereby reducing the probability of selecting such actions during policy updates.

Therefore, from the standard theoretical perspective of "objective function + penalty terms":

- **Reduced distance loss**: Indicates that the agent tends to choose action sequences that maintain safer distances from obstacles, thus reducing risk penalties due to collisions or proximity in terms of (long-term cumulative) value.
- **Reduced similarity loss**: Indicates that the agent is closer to the target image feature distribution, obtaining higher rewards for reaching the goal or lower penalties for navigation deviations.

Once these two losses are integrated into the value/advantage function calculation as negative rewards, PPO updates will explicitly penalize or reward-correct them, making policy directions with smaller penalties (smaller distance loss, smaller similarity loss) gain more sampling benefits, eventually dominating the policy iteration.

> **Conclusion**: As long as the newly added penalty terms (distance and similarity losses) satisfy the condition of "producing significant negative incentives for violations (or insufficient safety distance, proximity to obstacles) of state-action pairs," the corresponding gradient information affecting policy updates will tend to suppress high-risk or low-efficiency behaviors, ensuring convergence toward improved safety and efficiency within a certain range.

### 2.2 Relationship Between Convergence and Constraint Satisfaction

In the field of Constrained Reinforcement Learning (Constrained RL), similar theoretical discussions exist:

- If constraint term $\mathcal{C}(s,a)$ is introduced with the expectation of satisfying $ùîº[\sum_t \gamma^t \mathcal{C}(s_t,a_t)] \leq \delta$, it can be integrated into the objective function through Lagrangian multiplier $\lambda$.
- When the Lagrangian factor $\lambda$ is dynamically adjusted to an appropriate level, the final optimal policy will maximize the primary task reward while satisfying the constraints.
- Our method can be viewed as a variant of constrained reinforcement learning, where $\beta _1$ and $\beta _2$ play roles similar to Lagrangian multipliers, balancing the primary reward optimization against the satisfaction of distance and similarity constraints.

Applying this to our example, distance loss and similarity loss can be viewed as two types of "soft constraints," whose "feasible" or "better" states naturally meet the expected satisfaction (smaller losses mean safer and more efficient). Therefore, by balancing the loss terms in PPO and continuously updating the Lagrangian multiplier coefficients (if implemented through methods such as multi-objective RL or Constrained PPO), the converging policy can theoretically guarantee a balance between safety and efficiency.

### 2.3 Intuitive Explanation and Geometric Perspective

1. **Reduction of Distance Loss**:
   When distance loss decreases, it means we explicitly encourage the agent to select behavioral paths that are farther from obstacles, thereby lowering the value of "states-actions close to obstacles" in the value function. This is similar to applying negative rewards to collision or "dangerous" states in traditional RL.
2. **Reduction of Similarity Loss**:
   In the process of aligning with target image features, when feature similarity loss (difference) decreases, it is equivalent to "more likely approaching the target state." This corresponds to providing higher rewards for "states with small differences from target image features" in the value function, thus guiding the policy in that direction.

From a geometric intuition, these two constraints limit the feasible region of the agent's trajectory (farther from obstacles, closer to the target), equivalent to eliminating most of the search range that conflicts with constraints in the policy space, concentrating the effective search space towards "safe and efficient" regions, thus theoretically enabling faster and more stable convergence.

## 3. Empirical Support

Your experimental results also confirm the effectiveness of these theoretical analyses:

1. As shown in Table 4 of the paper, removing the distance constraint ($L_{dist}$) results in a 10.28% decrease in success rate (SR) and a 9.38% decrease in path efficiency (SPL); removing the similarity constraint ($L_{sim}$) results in a 3.58% decrease in success rate and a 6.66% decrease in path efficiency.
2. As shown in Table 1, compared to baseline methods, after introducing these two constraints, the number of training epochs required to achieve a 90% success rate decreased by an average of 36.41%, indicating that the constraints indeed help improve learning efficiency.
3. The trajectory comparison in Figure 1 and [Visualizations](https://github.com/AnonymousAccount-6F03/ICML-Visualizations-16380) visually demonstrates that navigation paths with constraints added are more direct and safer.

## 4. Summary

From the theoretical perspective of reinforcement learning, adding distance loss and similarity loss as "heuristic penalties" to the original PPO objective function **is not merely an empirical technique**, but can be theoretically explained or proven through the following points:

1. **Objective Function Modification**: Incorporating losses as negative rewards into the value/advantage function creates a new objective $\widetilde{J}(\theta)$. As distance loss and similarity loss decrease, this objective's expected value increases, directing PPO's gradient updates toward "safe and closer to target" directions.
2. **Constrained Reinforcement Learning Perspective**: These can be viewed as two soft constraints, which gradually satisfy the trade-off between safety and efficiency through appropriate Lagrangian multiplier or loss weight adjustments during training iterations. This is theoretically consistent with existing conclusions in Constrained RL regarding safety constraint convergence.
3. **Convergence Stability**: Benefiting from PPO's limitations on the magnitude of policy changes during updates, these additional penalty terms can smoothly modify the policy search space, eliminating high-risk or low-efficiency behaviors, thereby better converging to policies that both satisfy safety requirements and achieve target proximity.

Therefore, smaller distance loss (safer) and smaller similarity loss (more efficiently approaching the target) can be theoretically explained as: the corresponding constraint penalties are minimized in the value function, thus making positive contributions to overall policy improvement, ultimately reflected in improved navigation success rates and safety.
